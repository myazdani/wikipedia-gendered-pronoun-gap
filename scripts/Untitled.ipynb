{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 26 days\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import *\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"../data/wikitext-103/wiki.test.tokens\")\n",
    "txt_test = f.read()\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/wikitext-103/wiki.valid.tokens\")\n",
    "txt_valid = f.read()\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open(\"../data/wikitext-103/wiki.train.tokens\")\n",
    "txt_train = f.read()\n",
    "f.close()\n",
    "\n",
    "txt = txt_test + txt_valid + txt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters = re.finditer(r\"\\n = [\\w']+ .* = \\n \\n\", txt.lower())\n",
    "indices = [m.start(0) for m in iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles we have: 25951\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "for i, index in enumerate(indices):\n",
    "    if i < (len(indices)-1):\n",
    "        articles.append(txt[indices[i]:indices[i+1]])\n",
    "    else:\n",
    "        articles.append(txt[index:])\n",
    "        \n",
    "print \"The number of articles we have:\", len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import load\n",
    "_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "tagger = load(_POS_TAGGER)  # same tagger as using nltk.pos_tag\n",
    "\n",
    "regexp_tagger = nltk.tag.RegexpTagger([(r'\\(|\\)', '--')], backoff = tagger)\n",
    "\n",
    "def get_concordance(input_text, target_word, left_margin = 10, right_margin = 10):\n",
    "    text = nltk.Text(nltk.word_tokenize(input_text.decode('utf-8')))\n",
    "    \n",
    "    c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
    "\n",
    "    concordance_txt = ([text.tokens[map(lambda x: x-5 if (x-left_margin)>0 else 0,[offset])[0]:offset+right_margin] for offset in c.offsets(target_word)])\n",
    "    return [''.join([x+' ' for x in con_sub]) for con_sub in concordance_txt] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_titles = []\n",
    "first_sentence_articles = []\n",
    "\n",
    "he_counts = []\n",
    "she_counts = []\n",
    "word_counts = []\n",
    "\n",
    "all_he_words = []\n",
    "all_she_words = []\n",
    "\n",
    "he_concordances = []\n",
    "she_concordances = []\n",
    "pronoun_concordances = []\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    # get the title of the article\n",
    "    article_title = re.search(r\"\\n = [\\w']+ .* =\", article.lower()).group()[4:-2]\n",
    "    article_titles.append(article_title)\n",
    "    # get the first sentence of article\n",
    "    if i == 18881:\n",
    "        # it appears that this article is not a full article and only has one sentence \n",
    "        # without a period, so we treat it as a special case\n",
    "        first_sentence = article\n",
    "    else:\n",
    "        first_sentence = re.search(r'([A-Z][^\\.!?]*[\\.!?])', article).group()\n",
    "    # remove non-word characters\n",
    "    first_sentence = \" \".join(re.findall(\"[a-zA-Z()]+\", first_sentence))\n",
    "    # remove contents parenthesis and the content within it\n",
    "    first_sentence = re.sub(r'\\([^)]*\\)', '', first_sentence)\n",
    "    # remove title duplicacy\n",
    "    first_sentence = article_title + \" \" + re.sub(article_title.lower(), '', first_sentence.lower()).strip()\n",
    "    first_sentence_articles.append(first_sentence)\n",
    "    \n",
    "    # compute pronoun stats\n",
    "    he_counts.append(article.lower().count(\" he \"))\n",
    "    she_counts.append(article.lower().count(\" she \"))\n",
    "    word_counts.append(len(re.findall(\"[a-zA-Z_]+\", article)))\n",
    "    \n",
    "    words_after_he = \" \".join(re.findall(r\" he [\\w']+\", article.lower())) \n",
    "    all_he_words.append(re.sub(r\"\\bhe\\b\", '', words_after_he).strip())\n",
    "    \n",
    "    words_after_she = \" \".join(re.findall(r\" she [\\w']+\", article.lower())) \n",
    "    all_she_words.append(re.sub(r\"\\bshe\\b\", '', words_after_she).strip())\n",
    "    \n",
    "    # get pronoun concordances\n",
    "    he_concordance = \" \".join(get_concordance(article, target_word = \"HE\"))\n",
    "    she_concordance = \" \".join(get_concordance(article, target_word = \"SHE\"))\n",
    "    \n",
    "    he_concordances.append(he_concordance)\n",
    "    she_concordances.append(she_concordance)\n",
    "    pronoun_concordances.append(he_concordance+she_concordance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>first_sentence</th>\n",
       "      <th>he_counts</th>\n",
       "      <th>she_counts</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>words_after_he</th>\n",
       "      <th>words_after_she</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25946</th>\n",
       "      <td>al wistert</td>\n",
       "      <td>al wistert albert alexander ox wistert  was an...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>played   was   was   is   was   was   wore   w...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25947</th>\n",
       "      <td>si una vez</td>\n",
       "      <td>si una vez is a song recorded by american reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1064</td>\n",
       "      <td></td>\n",
       "      <td>ever   will   ever   said   felt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25948</th>\n",
       "      <td>sicklefin lemon shark</td>\n",
       "      <td>sicklefin lemon shark the  or sharptooth lemon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2784</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25949</th>\n",
       "      <td>ontario highway 89</td>\n",
       "      <td>ontario highway 89 ontario highway king s high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>699</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25950</th>\n",
       "      <td>luke smith ( writer )</td>\n",
       "      <td>luke smith ( writer ) luke smith  luke michael...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1120</td>\n",
       "      <td>is   wrote   considered   left   worked   left...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_title  \\\n",
       "25946             al wistert   \n",
       "25947             si una vez   \n",
       "25948  sicklefin lemon shark   \n",
       "25949     ontario highway 89   \n",
       "25950  luke smith ( writer )   \n",
       "\n",
       "                                          first_sentence  he_counts  \\\n",
       "25946  al wistert albert alexander ox wistert  was an...         29   \n",
       "25947  si una vez is a song recorded by american reco...          0   \n",
       "25948  sicklefin lemon shark the  or sharptooth lemon...          0   \n",
       "25949  ontario highway 89 ontario highway king s high...          0   \n",
       "25950  luke smith ( writer ) luke smith  luke michael...         18   \n",
       "\n",
       "       she_counts  word_counts  \\\n",
       "25946           0          912   \n",
       "25947           5         1064   \n",
       "25948           0         2784   \n",
       "25949           0          699   \n",
       "25950           0         1120   \n",
       "\n",
       "                                          words_after_he  \\\n",
       "25946  played   was   was   is   was   was   wore   w...   \n",
       "25947                                                      \n",
       "25948                                                      \n",
       "25949                                                      \n",
       "25950  is   wrote   considered   left   worked   left...   \n",
       "\n",
       "                        words_after_she  \n",
       "25946                                    \n",
       "25947  ever   will   ever   said   felt  \n",
       "25948                                    \n",
       "25949                                    \n",
       "25950                                    "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pronoun_stats = pd.DataFrame({'article_title': article_titles, 'first_sentence': first_sentence_articles,\n",
    "                                'he_counts': he_counts, 'she_counts': she_counts, 'word_counts': word_counts,\n",
    "                                \"words_after_he\": all_he_words, \"words_after_she\": all_she_words})\n",
    "df_pronoun_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25951, 79507)\n",
      "(25951, 79507)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df = 10, analyzer = \"word\", stop_words=\"english\")\n",
    "X_train_counts = count_vect.fit_transform(articles)\n",
    "print X_train_counts.shape\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=4, n_iter=7, random_state=42)\n",
    "svd_articles = svd.fit_transform(X_train_tf)\n",
    "articles_svd_df = pd.DataFrame(svd_articles)\n",
    "articles_svd_df.rename(columns=lambda x: \"pc\" + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25951, 22109)\n",
      "(25951, 22109)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df = 10, analyzer = \"word\", stop_words=\"english\")\n",
    "X_train_counts = count_vect.fit_transform(pronoun_concordances)\n",
    "print X_train_counts.shape\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd_pronoun_words = svd.fit_transform(X_train_tf)\n",
    "pronoun_svd_df = pd.DataFrame(svd_pronoun_words)\n",
    "pronoun_svd_df.rename(columns=lambda x: \"pronoun_pc\" + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd_df = articles_svd_df.join(pronoun_svd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>first_sentence</th>\n",
       "      <th>he_counts</th>\n",
       "      <th>she_counts</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>words_after_he</th>\n",
       "      <th>words_after_she</th>\n",
       "      <th>pc0</th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pronoun_pc0</th>\n",
       "      <th>pronoun_pc1</th>\n",
       "      <th>pronoun_pc2</th>\n",
       "      <th>pronoun_pc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25941</th>\n",
       "      <td>peterborough ( uk parliament constituency )</td>\n",
       "      <td>peterborough ( uk parliament constituency ) pe...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1761</td>\n",
       "      <td>defeated   held   lost   lost   was   then</td>\n",
       "      <td>was   had   had</td>\n",
       "      <td>0.217689</td>\n",
       "      <td>-0.086183</td>\n",
       "      <td>-0.058102</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.128556</td>\n",
       "      <td>-0.056635</td>\n",
       "      <td>-0.027908</td>\n",
       "      <td>-0.029251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25942</th>\n",
       "      <td>bart 's girlfriend</td>\n",
       "      <td>bart 's girlfriend bart s girlfriend bart s gi...</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>1847</td>\n",
       "      <td>discovers   is   was   approaches   is   is   ...</td>\n",
       "      <td>is   ignores   still   likes   seems   tells  ...</td>\n",
       "      <td>0.176964</td>\n",
       "      <td>0.149496</td>\n",
       "      <td>-0.073557</td>\n",
       "      <td>-0.150156</td>\n",
       "      <td>0.183882</td>\n",
       "      <td>0.108048</td>\n",
       "      <td>-0.001484</td>\n",
       "      <td>-0.019044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25943</th>\n",
       "      <td>delhi metro</td>\n",
       "      <td>delhi metro the  is a metro system serving del...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5599</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.376350</td>\n",
       "      <td>-0.150125</td>\n",
       "      <td>0.078491</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25944</th>\n",
       "      <td>time enough at last</td>\n",
       "      <td>time enough at last is episode of the american...</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2319</td>\n",
       "      <td>wastes   eagerly   sees   sees   prepares   ha...</td>\n",
       "      <td>has   destroys</td>\n",
       "      <td>0.347325</td>\n",
       "      <td>0.165617</td>\n",
       "      <td>-0.058324</td>\n",
       "      <td>-0.259688</td>\n",
       "      <td>0.149348</td>\n",
       "      <td>0.032326</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.009463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25945</th>\n",
       "      <td>dead head fred</td>\n",
       "      <td>dead head fred is a horror themed action adven...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3611</td>\n",
       "      <td>pieces   relies   can   was   could   does   d...</td>\n",
       "      <td></td>\n",
       "      <td>0.260642</td>\n",
       "      <td>0.081734</td>\n",
       "      <td>-0.045168</td>\n",
       "      <td>-0.174923</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.015098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25946</th>\n",
       "      <td>al wistert</td>\n",
       "      <td>al wistert albert alexander ox wistert  was an...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>played   was   was   is   was   was   wore   w...</td>\n",
       "      <td></td>\n",
       "      <td>0.260348</td>\n",
       "      <td>0.024139</td>\n",
       "      <td>-0.122821</td>\n",
       "      <td>-0.188250</td>\n",
       "      <td>0.266892</td>\n",
       "      <td>-0.214507</td>\n",
       "      <td>0.277040</td>\n",
       "      <td>0.096868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25947</th>\n",
       "      <td>si una vez</td>\n",
       "      <td>si una vez is a song recorded by american reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1064</td>\n",
       "      <td></td>\n",
       "      <td>ever   will   ever   said   felt</td>\n",
       "      <td>0.245286</td>\n",
       "      <td>0.325969</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.269059</td>\n",
       "      <td>0.149783</td>\n",
       "      <td>0.210442</td>\n",
       "      <td>0.060708</td>\n",
       "      <td>-0.075083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25948</th>\n",
       "      <td>sicklefin lemon shark</td>\n",
       "      <td>sicklefin lemon shark the  or sharptooth lemon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2784</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.281761</td>\n",
       "      <td>-0.120124</td>\n",
       "      <td>0.101113</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25949</th>\n",
       "      <td>ontario highway 89</td>\n",
       "      <td>ontario highway 89 ontario highway king s high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>699</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.119769</td>\n",
       "      <td>-0.110401</td>\n",
       "      <td>-0.099086</td>\n",
       "      <td>0.075366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25950</th>\n",
       "      <td>luke smith ( writer )</td>\n",
       "      <td>luke smith ( writer ) luke smith  luke michael...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1120</td>\n",
       "      <td>is   wrote   considered   left   worked   left...</td>\n",
       "      <td></td>\n",
       "      <td>0.180736</td>\n",
       "      <td>0.102987</td>\n",
       "      <td>-0.080158</td>\n",
       "      <td>-0.152136</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.120769</td>\n",
       "      <td>0.105034</td>\n",
       "      <td>-0.038383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     article_title  \\\n",
       "25941  peterborough ( uk parliament constituency )   \n",
       "25942                           bart 's girlfriend   \n",
       "25943                                  delhi metro   \n",
       "25944                          time enough at last   \n",
       "25945                               dead head fred   \n",
       "25946                                   al wistert   \n",
       "25947                                   si una vez   \n",
       "25948                        sicklefin lemon shark   \n",
       "25949                           ontario highway 89   \n",
       "25950                        luke smith ( writer )   \n",
       "\n",
       "                                          first_sentence  he_counts  \\\n",
       "25941  peterborough ( uk parliament constituency ) pe...          6   \n",
       "25942  bart 's girlfriend bart s girlfriend bart s gi...         20   \n",
       "25943  delhi metro the  is a metro system serving del...          0   \n",
       "25944  time enough at last is episode of the american...         23   \n",
       "25945  dead head fred is a horror themed action adven...         12   \n",
       "25946  al wistert albert alexander ox wistert  was an...         29   \n",
       "25947  si una vez is a song recorded by american reco...          0   \n",
       "25948  sicklefin lemon shark the  or sharptooth lemon...          0   \n",
       "25949  ontario highway 89 ontario highway king s high...          0   \n",
       "25950  luke smith ( writer ) luke smith  luke michael...         18   \n",
       "\n",
       "       she_counts  word_counts  \\\n",
       "25941           3         1761   \n",
       "25942          24         1847   \n",
       "25943           0         5599   \n",
       "25944           2         2319   \n",
       "25945           0         3611   \n",
       "25946           0          912   \n",
       "25947           5         1064   \n",
       "25948           0         2784   \n",
       "25949           0          699   \n",
       "25950           0         1120   \n",
       "\n",
       "                                          words_after_he  \\\n",
       "25941         defeated   held   lost   lost   was   then   \n",
       "25942  discovers   is   was   approaches   is   is   ...   \n",
       "25943                                                      \n",
       "25944  wastes   eagerly   sees   sees   prepares   ha...   \n",
       "25945  pieces   relies   can   was   could   does   d...   \n",
       "25946  played   was   was   is   was   was   wore   w...   \n",
       "25947                                                      \n",
       "25948                                                      \n",
       "25949                                                      \n",
       "25950  is   wrote   considered   left   worked   left...   \n",
       "\n",
       "                                         words_after_she       pc0       pc1  \\\n",
       "25941                                    was   had   had  0.217689 -0.086183   \n",
       "25942  is   ignores   still   likes   seems   tells  ...  0.176964  0.149496   \n",
       "25943                                                     0.376350 -0.150125   \n",
       "25944                                     has   destroys  0.347325  0.165617   \n",
       "25945                                                     0.260642  0.081734   \n",
       "25946                                                     0.260348  0.024139   \n",
       "25947                   ever   will   ever   said   felt  0.245286  0.325969   \n",
       "25948                                                     0.281761 -0.120124   \n",
       "25949                                                     0.119769 -0.110401   \n",
       "25950                                                     0.180736  0.102987   \n",
       "\n",
       "            pc2       pc3  pronoun_pc0  pronoun_pc1  pronoun_pc2  pronoun_pc3  \n",
       "25941 -0.058102  0.018380     0.128556    -0.056635    -0.027908    -0.029251  \n",
       "25942 -0.073557 -0.150156     0.183882     0.108048    -0.001484    -0.019044  \n",
       "25943  0.078491  0.046512     0.000000     0.000000     0.000000     0.000000  \n",
       "25944 -0.058324 -0.259688     0.149348     0.032326    -0.021729     0.009463  \n",
       "25945 -0.045168 -0.174923     0.064506     0.019422     0.000329     0.015098  \n",
       "25946 -0.122821 -0.188250     0.266892    -0.214507     0.277040     0.096868  \n",
       "25947  0.046810  0.269059     0.149783     0.210442     0.060708    -0.075083  \n",
       "25948  0.101113  0.034368     0.000000     0.000000     0.000000     0.000000  \n",
       "25949 -0.099086  0.075366     0.000000     0.000000     0.000000     0.000000  \n",
       "25950 -0.080158 -0.152136     0.216797     0.120769     0.105034    -0.038383  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = df_pronoun_stats.join(svd_df)\n",
    "res.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.to_csv(\"../data/wiki_tfidf.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
